---
- hosts: localhost
  gather_facts: no
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
  - include_vars: config.yml
  - name: Launch instance
    ec2:
       key_name: "{{ key_name }}"
       group: "{{ security_group }}"
       instance_type: "{{ instance_type }}"
       image: "{{ image }}"
       count: "{{ count }}"
       vpc_subnet_id: "{{ vpc_subnet }}"
       region: "{{ region }}"
       wait: true
       assign_public_ip: yes
       user_data: |
         #cloud-config
         manage_etc_hosts: localhost
         system_info:
           default_user:
             name: "{{ user }}"
    register: ec2

  - name: Add public ip of new instances to host group
    add_host:
      hostname: "{{ item.public_ip }}"
      groupname: public 
    with_items: "{{ ec2.instances }}"

  - name: Add public ip of master to host group
    add_host:
      hostname: "{{ ec2.instances.0.public_ip }}"
      groupname: master

  - name: Add private ip of new instances to host group
    add_host:
      hostname: "{{ item.private_ip }}"
      groupname: private
    with_items: "{{ ec2.instances }}"

  - name: Wait for SSH to come up
    wait_for:
      host: "{{ item.public_ip }}"
      port: 22
      delay: 60
      timeout: 320
      state: started
    with_items: "{{ ec2.instances }}"

##########################################

- hosts: public
  gather_facts: no
  vars: 
    ansible_python_interpreter: /usr/bin/python3
  tasks:
  - include_vars: config.yml

  # boto

  - name: boto.cfg
    copy:
      dest: /etc/boto.cfg
      content: |
        [Credentials]
        aws_access_key_id={{ lookup('pipe', 'aws configure --profile {{ profile }} get aws_access_key_id') }}
        aws_secret_access_key={{ lookup('pipe', 'aws configure --profile {{ profile }} get aws_secret_access_key') }}
    become: yes

  # hosts

  - name: hostnames
    lineinfile:
      dest: /etc/hosts
      line: "{{ item.1 }} node{{ item.0 }}"
    with_indexed_items: "{{ groups.private }}"
    become: yes

  # packages

  - name: update cache
    apt:
      update_cache: yes
    become: yes
  - name: install java on all nodes
    apt:
      package: "{{ item }}"
      state: latest
    become: yes
    with_items:
    - openjdk-8-jre-headless
    - awscli
    - r-base

  # files

  - name: .ssh/config
    copy: 
      dest: .ssh/config
      content: |
        StrictHostKeyChecking no
  - name: .screenrc
    copy: 
      dest: ./.screenrc
      content: |
        escape ^Oo
        termcapinfo xterm* ti@:te@
        bind x
        bind ^x
  - name: copy .ssh/id_rsa
    copy: 
      src: ~/.ssh/id_rsa
      dest: .ssh/id_rsa
      mode: 0600
  - name: .profile
    lineinfile:
      dest: ~/.profile 
      line: "{{ item }}"
    with_items: 
    - PATH="/usr/local/lib/spark/bin:$PATH"
    - PATH="/usr/local/lib/hadoop/bin:$PATH"
    - PATH="/usr/local/lib/hive/bin:$PATH"
    - export YARN_CONF_DIR=/usr/local/lib/hadoop/etc/hadoop
    - export PYSPARK_PYTHON=python3
    - '[ -z "$STY" -a -n "$SSH_TTY" ] && exec screen -RR'

  # hadoop

  - name: download hadoop
    get_url:
      url: http://mirror.cc.columbia.edu/pub/software/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
      dest: ./hadoop.tgz
  - name: create hadoop directory
    file: 
      dest: /usr/local/lib/hadoop
      state: directory
      owner: "{{ user }}"
    become: yes
  - name: untar hadoop
    unarchive:
      src: /home/{{ user }}/hadoop.tgz
      dest: /usr/local/lib/hadoop
      extra_opts: --strip-components=1
      remote_src: yes
  - name: remove hadoop archive
    file: dest=hadoop.tgz state=absent

  # hive

  - name: download hive
    get_url:
      url: http://mirror.cc.columbia.edu/pub/software/apache/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz
      dest: ./hive.tgz
  - name: create hive directory
    file: 
      dest: /usr/local/lib/hive
      state: directory
      owner: "{{ user }}"
    become: yes
  - name: untar hive 
    unarchive:
      src: /home/{{ user }}/hive.tgz
      dest: /usr/local/lib/hive
      extra_opts: --strip-components=1
      remote_src: yes
  - name: remove hive archive
    file: dest=hive.tgz state=absent

  # spark

  - name: download spark
    get_url:
      url: http://mirror.cc.columbia.edu/pub/software/apache/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz
      dest: ./spark.tgz
  - name: create spark directory
    file: 
      dest: /usr/local/lib/spark 
      state: directory
      owner: "{{ user }}"
    become: yes
  - name: untar spark
    unarchive:
      src: /home/{{ user }}/spark.tgz
      dest: /usr/local/lib/spark
      extra_opts: --strip-components=1
      remote_src: yes
  - name: remove spark archive
    file: dest=spark.tgz state=absent

  # configure spark

  - name: slaves
    lineinfile:
      dest: /usr/local/lib/spark/conf/slaves
      line: "{{ item }}"
      create: yes
    with_items: "{{ groups.private[1:] }}"
  - name: master
    copy:
      dest: /usr/local/lib/spark/conf/spark-env.sh
      content: |
         export SPARK_MASTER_HOST={{ groups.private[0] }}
         export PYTHONHASHSEED=0

  # configure hadoop

  - file: 
      dest: "{{ data_path }}/dfs"
      state: directory
      owner: "{{ user }}" 
    become: yes
  - file: 
      dest: "{{ data_path }}/yarn"
      state: directory
      owner: "{{ user }}"
    become: yes
  - file: 
      dest: /usr/local/lib/hadoop/etc/hadoop/slaves
      state: absent
  - name: slaves
    lineinfile:
      dest: /usr/local/lib/hadoop/etc/hadoop/slaves
      line: "{{ item }}"
      create: yes
    with_items: "{{ groups.private[1:] }}"
  - name: hadoop-env.sh
    lineinfile:
      dest: /usr/local/lib/hadoop/etc/hadoop/hadoop-env.sh
      line: "{{ item }}"
    with_items: 
      - export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
      - export PYTHONHASHSEED=0
  - name: hdfs-site.xml
    lineinfile:
      dest: /usr/local/lib/hadoop/etc/hadoop/hdfs-site.xml
      line: <property>{{ item }}</property>
      insertbefore: </configuration>
    with_items: 
    - <name>dfs.name.dir</name><value>file://{{ data_path }}/dfs/nn</value>
    - <name>dfs.data.dir</name><value>file://{{ data_path }}/dfs/dn</value>
    - <name>dfs.namenode.checkpoint.dir</name><value>file://{{ data_path }}/dfs/snn</value>
  - name: core-site.xml
    lineinfile:
      dest: /usr/local/lib/hadoop/etc/hadoop/core-site.xml
      line: <property>{{ item }}</property>
      insertbefore: </configuration>
    with_items: 
    - <name>fs.defaultFS</name><value>hdfs://{{ groups.private[0] }}</value>
  - name: mapred-site.xml
    copy:
      dest: /usr/local/lib/hadoop/etc/hadoop/mapred-site.xml
      content: |
        <configuration>
        </configuration>
  - name: mapred-site.xml
    lineinfile:
      dest: /usr/local/lib/hadoop/etc/hadoop/mapred-site.xml
      line: <property>{{ item }}</property>
      insertbefore: </configuration>
    with_items: 
    - <name>mapreduce.framework.name</name><value>yarn</value>
    - <name>mapreduce.map.memory.mb</name><value>{{ map_memory_mb }}</value>
    - <name>mapreduce.map.java.opts</name><value>{{ map_memory_opts }}</value>
    - <name>mapreduce.reduce.memory.mb</name><value>{{ reduce_memory_mb }}</value>
    - <name>mapreduce.reduce.java.opts</name><value>{{ reduce_memory_opts }}</value>
    - <name>mapreduce.task.io.sort.mb</name><value>{{ sort_memory_mb }}</value>
    - <name>yarn.app.mapreduce.am.resource.mb</name><value>{{ am_memory_mb }}</value>
    - <name>yarn.app.mapreduce.am.command-opts</name><value>{{ am_memory_opts }}</value>
  - name: yarn-site.xml
    lineinfile:
      dest: /usr/local/lib/hadoop/etc/hadoop/yarn-site.xml
      line: <property>{{ item }}</property>
      insertbefore: </configuration>
    with_items: 
    - <name>yarn.scheduler.minimum-allocation-mb</name><value>{{ yarn_minimum_mb }}</value>
    - <name>yarn.scheduler.maximum-allocation-mb</name><value>{{ yarn_maximum_mb }}</value>
    - <name>yarn.nodemanager.resource.memory-mb</name><value>{{ yarn_maximum_mb }}</value>
    - <name>yarn.resourcemanager.hostname</name><value>localhost</value>
    - <name>yarn.nodemanager.local-dirs</name><value>{{ data_path }}/yarn</value>
    - <name>yarn.nodemanager.aux-services</name><value>mapreduce_shuffle</value>
    - <name>yarn.resourcemanager.hostname</name><value>{{ groups.private[0] }}</value>
    - <name>yarn.nodemanager.hostname</name><value>node{{ play_hosts.index(inventory_hostname) }}</value>

  # configure hive

  - file: 
      dest: "{{ data_path }}/hive"
      state: directory
      owner: "{{ user }}"
    become: yes
  - name: hive-site.xml
    copy:
      dest: /usr/local/lib/hive/conf/hive-site.xml
      content: |
        <configuration>
        </configuration>
  - name: mapred-site.xml
    lineinfile:
      dest: /usr/local/lib/hive/conf/hive-site.xml
      line: <property>{{ item }}</property>
      insertbefore: </configuration>
    with_items: 
    - <name>hive.metastore.warehouse.dir</name><value>{{ data_path }}/hive</value>
    - <name>javax.jdo.option.ConnectionURL</name><value>jdbc:derby:;databaseName={{ data_path }}/hive/metastore_db;create=true</value>

##########################################

- hosts: master
  gather_facts: no
  vars: 
    ansible_python_interpreter: /usr/bin/python3
  tasks:
  - include_vars: config.yml
  - name: install packages on master
    apt:
      package: "{{ item }}"
      state: latest
    become: yes
    with_items:
    - tinyproxy
    - python3-setuptools
    - python3-dev
    - jq
  - name: const.txt
    get_url:
      url: http://www.usconstitution.net/const.txt
      dest: .

  # git

  - name: git init
    command: git init --bare .git
  - name: git hook
    copy: 
      dest: .git/hooks/post-receive 
      mode: 0775
      content: |
        while read oldrev newrev ref; do
          echo $ref
          if [ "$ref" = "refs/heads/master" ]; then
            export GIT_WORK_TREE=$(dirname $PWD)
            git clean -f -d -e "/.*" $GIT_WORK_TREE
            git checkout -f master
          fi
        done

  # jupyter

  - name: install pip
    command: easy_install3 pip
    become: yes
  - name: install jupyter
    command: pip install jupyter
    become: yes
  - name: .jupyter/
    file:
      dest: ./.jupyter
      state: directory
  - name: .jupyter/jupyter_notebook_config.py 
    copy:
      dest: .jupyter/jupyter_notebook_config.py 
      content: |
        c.NotebookApp.open_browser = False
        c.NotebookApp.port = 8000
        c.NotebookApp.token = ''

  # sparkR

  - name: install R packages on master
    apt:
      package: "{{ item }}"
      state: latest
    become: yes
    with_items:
    - openjdk-8-jdk-headless
    - r-base-dev
    - libssl-dev           # for devtools
    - libssh2-1-dev        # for devtools
    - libcurl4-openssl-dev # for devtools
  - name: build irkernel
    command: Rscript -e "{{ item }}"
    with_items:
    - install.packages(c('repr', 'IRdisplay', 'crayon', 'pbdZMQ', 'devtools'), repos='http://cran.us.r-project.org')
    - devtools::install_github('IRkernel/IRkernel')
    - install.packages(c('bitops', 'RCurl'), repos='http://cran.us.r-project.org')
    become: yes
  - name: install irkernel
    command: Rscript -e "IRkernel::installspec()"

  # spark

  - name: submit.sh
    copy:
      dest: ./submit.sh
      content: |
        spark-submit --master spark://{{ groups.private[0] }}:7077 $@
  - name: wc.R
    copy:
      dest: ./wc.R
      content: |
        library(SparkR)
        sparkR.session()
        df <- as.DataFrame(faithful)
        count(df)
        head(select(df, df$eruptions))
  - name: wc.py
    copy:
      dest: ./wc.py
      content: |
        from pyspark import SparkContext
        sc = SparkContext(appName="wc")
        lines = sc.textFile('const.txt')
        lines_nonempty = lines.filter( lambda x: len(x) > 0 )
        words = lines_nonempty.flatMap(lambda x: x.split())
        wordcounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False)
        print(wordcounts.take(10))
  - name: wc.sh
    copy:
      dest: ./wc.sh
      content: |
        hadoop fs -put const.txt .
        spark-submit --master spark://{{ groups.private[0] }}:7077 wc.py
        echo press return to continue; read INPUT
        spark-submit --master yarn wc.py
        echo press return to continue; read INPUT
        spark-submit --master yarn --deploy-mode cluster wc.py
  - name: notebook.sh
    copy:
      dest: ./notebook.sh
      content: |
        export PYSPARK_DRIVER_PYTHON=jupyter
        export PYSPARK_DRIVER_PYTHON_OPTS=notebook
        pyspark --master spark://{{ groups.private[0] }}:7077

  # start spark

  - name: start spark
    command: /usr/local/lib/spark/sbin/start-all.sh

  # hive

  - copy:
      dest: ./hive.sh
      content: |
        hive <<EOF
        create table dummy (value string);
        load data local inpath 'const.txt' overwrite into table dummy;
        select count(*) from dummy;
        show tables;
        describe dummy;
        EOF

  # hadoop

  - copy:
      dest: ./mapreduce.sh
      content: |
        hadoop fs -mkdir input
        hadoop fs -put const.txt input
        hadoop fs -ls input
        hadoop jar /usr/local/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount input output
        hadoop fs -lsr

  # start hadoop

  - name: format hdfs
    command: /usr/local/lib/hadoop/bin/hadoop namenode -format
  - name: start dfs
    command: /usr/local/lib/hadoop/sbin/start-dfs.sh
  - name: create tmp directory in hdfs
    command: /usr/local/lib/hadoop/bin/hadoop fs -mkdir -p /tmp
  - name: create home directory in hdfs
    command: /usr/local/lib/hadoop/bin/hadoop fs -mkdir -p /user/{{ user }}
  - name: start yarn
    command: /usr/local/lib/hadoop/sbin/start-yarn.sh

##########################################

- hosts: localhost
  gather_facts: no
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
  - name: master.sh
    copy:
      dest: ./master.sh
      content: |
        ssh {{ groups.public[0] }}
  - name: master.sh
    lineinfile:
      dest: ./master.sh
      line: "# ssh {{ item }}"
    with_items: "{{ groups.public[1:] }}"
  - name: proxy.sh
    copy:
      dest: ./proxy.sh
      content: |
        google-chrome --proxy-server=localhost:8888 {{ groups.private[0] }}:8080 &  # spark
        google-chrome --proxy-server=localhost:8888 {{ groups.private[0] }}:8088 &  # yarn RM
        google-chrome --proxy-server=localhost:8888 {{ groups.private[0] }}:50070 & # namenode
        ssh -N -L 8888:localhost:8888 {{ groups.public[0] }}
  - name: proxy.sh
    lineinfile:
      dest: ./proxy.sh
      line: google-chrome --proxy-server=localhost:8888 {{ item }}:8042 & # yarn NM
      insertbefore: ssh
    with_items: "{{ groups.private[1:] }}"
  - name: notebook.sh
    copy:
      dest: ./notebook.sh
      content: |
        google-chrome http://localhost:8000 &
        ssh -N -L 8000:localhost:8000 {{ groups.public[0] }}
  - name: push.sh
    copy:
      dest: ./push.sh
      content: |
        git push ssh://{{ groups.public[0] }}/~/.git master

